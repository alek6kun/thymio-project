{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241f2d56-55bf-4a51-acff-6bcfd647551a",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660af14b-7a62-4608-98ef-5b4b9edc2cc9",
   "metadata": {},
   "source": [
    "## Table of Content:\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. General program flow](#General_program_flow)\n",
    "* [3. Computer Vision](#Computer_vision )\n",
    "    * [3.1 Robot detection and localisation](#31-robot-detection-and-localisation)\n",
    "    * [3.2 Obstacle Detection](#32-obstacle-detection)\n",
    "    * [3.3 Path Planning](#33-path-planning)\n",
    "    * [3.4 Goal Identification](#34-goal-identification)\n",
    "* [4. Filtering](#Filtering)\n",
    "    * [4.1 The state space model](#41-the-state-space-model)\n",
    "    * [4.2 Filter choice and parameters](#42-filter-choice-and-parameters)\n",
    "    * [4.3 Running the filter](#43-running-the-filter)\n",
    "* [5. General Motion](#General_motion)\n",
    "* [6. Local avoidance](#Local_avoidance)\n",
    "    * [6.1 Requirements on the obstacles for local avoidance](#61-requirements-on-the-obstacles-for-local-avoidance)\n",
    "    * [6.2 Local avoidance implementation](#62-local-avoidance-implementation)\n",
    "* [7. Kidnapping](#Kidnapping)\n",
    "* [8. Conclusion](#Conclusion)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34696f0d-2083-4d0c-9d66-d989480e8a71",
   "metadata": {},
   "source": [
    "\n",
    "## Project: Autonomous Navigation System for a Mobile Robot\n",
    "\n",
    "### Participants:\n",
    "- **Alexis Limozin  (330717)**\n",
    "- **Parth Aggarwal  (376263)**\n",
    "- **Serge El Asmar   (326623)**\n",
    "- **Quentin Ang√©loz (325935)**\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "The aim of our project is to manage the thymio's path to a target in an environment containing obstacles.\n",
    "Our project uses a camera to create a map of the environment, taking obstacles into account. The thymio plans its path based on this map, and travels to where the obstacle is. If the environment changes and the robot detects an unexpected obstacle, the camera generates a new map that takes the new obstacle into account, and the thymio corrects its route.\n",
    "## Assumptions and environment description\n",
    "- The environment is a plane.\n",
    "- The ground is only one color, not green, not black and not red.\n",
    "- Obstacles color is black\n",
    "- The goal color is green\n",
    "- Operate the camera, obstacles and robot in a controlled environment\n",
    "- The Thymio is identified by two unique pieces of red tape\n",
    "\n",
    "Our axis system is as follows:\n",
    "\n",
    "<img src=\"images/axis_system.jpg\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5619e",
   "metadata": {},
   "source": [
    "## Environment Visual\n",
    "\n",
    "<img src=\"images/setup.jpg\" width=\"550\" height=\"700\">\n",
    "\n",
    "## Running our code\n",
    "Our main function is located in `main.ipynb`. It consists of one cell, which, when ran runs the entire code. It may be necessary to read the `vision/README.md` file located in the vision folder prior to running `main.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8839e0",
   "metadata": {},
   "source": [
    "### The cell below must be run to visualize the videos included in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebdf78-3b70-4465-b65d-f463c1f1c1ec",
   "metadata": {},
   "source": [
    "## 2. General program flow <a class=\"anchor\" id=\"General_program_flow\"></a>\n",
    "### The overall program sequence is as follows:\n",
    "\n",
    "The robot has 4 different states, which are the body of the program.\n",
    "\n",
    "In each of these states, an image is periodically acquired.\n",
    "\n",
    "When the program starts, the robot is in state 0. \n",
    "- State 0\n",
    "    - The program searches for the robot until a path from the robot's position to the target can be calculated and stored in memory. Once this has been done, the robot switches to state 1.\n",
    "- State 1\n",
    "    - The robot follows the previously calculated path. It regularly corrects its trajectory using the Kalman filter.\n",
    "    - If the robot is kidnapped, it enters state 3.\n",
    "    - If the robot detects an unexpected obstacle, it enters state 2.\n",
    "    - If the robot has reached its goal, it stops.\n",
    "- State 2:\n",
    "    - the robot moves backwards until it can no longer see an obstacle, then returns to state 0.\n",
    "- State 3:\n",
    "    - the robot stops moving, and waits to be put back on the ground. When this is done, it returns to state 0.\n",
    "\n",
    "A simple diagram of the finite state machine is given below.\n",
    "\n",
    "<img src=\"images/FSM.jpg\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97aa90",
   "metadata": {},
   "source": [
    "### Use of asynchronous programming\n",
    "Asynchronous programming was used to send command signals to the motor, to read values from the robot's proximity sensors and to read values from the motor speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db8828-6401-4a63-85c9-f45081357a64",
   "metadata": {},
   "source": [
    "The following cell contains a video illustrating a simple case of the general program flow. In the video, the robot is never kidnapped, and no unplanned obstacle is being placed on its path, therefore, local navigation is never used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c89157-5bf6-4b47-b610-150fc98500ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"videos/normal_behavior.mp4\" type=\"video/mp4\">\n",
    "  Error, can't play video\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a588ff-c483-466d-adef-02133649f404",
   "metadata": {},
   "source": [
    "# 3. Computer vision <a class=\"anchor\" id=\"Computer_vision\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0565d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.ComputerVision import Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548983e7",
   "metadata": {},
   "source": [
    "To install the libraries used in vision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ac17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install pyvisgraph\n",
    "!pip install shapely"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76b3a800",
   "metadata": {},
   "source": [
    "The vision aspect of our project is designed to navigate the robot through an environment with obstacles utilising a stationary camera. Our approach follows the following steps:\n",
    "\n",
    "1. Detecting and locating the robot and its orientation.\n",
    "2. Identifying obstacles and their boundaries.\n",
    "3. Determining a goal position.\n",
    "4. Computing the shortest path for the robot to reach the goal while avoiding obstacles. \n",
    "\n",
    "### Libraries used to aid Vision \n",
    "- `OpenCV`: Used for image detection, contour detection, geometric transformations, utilised to process captured frames and propose the visual input to achieve the goal \n",
    "- `NumPy`: Utilised for all numerical operations\n",
    "- `Pyvisgraph`: Utilised for computing optimal robot paths based on all calculations \n",
    "- `Shapely`: Utilised for manipulating planar geometric objects (to create union between obstacles that are close together)\n",
    "\n",
    "Below is a code snippet showing how the vision code is organized using classes. In order to make the interfacing as seamless as possible, all the vision code is contained within the vision/ComputerVision.py file. There is one main class where each object attributes can be accessed. More detail in its usage in vision/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self, x, y, angle):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.angle = angle\n",
    "\n",
    "class Goal:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "# Main class used to interface with vision\n",
    "class Vision:\n",
    "    # Initialise cam, frame, copy of frame, robot, scale, graph,\n",
    "    # obstacle vertices, shortest_path\n",
    "    def __init__(self):\n",
    "        self.cam = cv2.VideoCapture(0)\n",
    "        valid, self.frame = self.cam.read()\n",
    "        if not valid:\n",
    "            print(\"Error reading frame.\")\n",
    "        self.copy = self.frame.copy()\n",
    "        self.found_robot, self.robot, self.scale = self.find_robot()\n",
    "        self.found_graph, self.vertices, self.graph = self.find_graph()\n",
    "        self.found_goal, self.goal = self.find_goal()\n",
    "        self.shortest_path = []\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09654a99",
   "metadata": {},
   "source": [
    "The simplest usage of the vision code without any robot control can be done with the following code snippet, in vision/test_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc89d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.ComputerVision import Vision\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "vis = Vision()\n",
    "# Initialize a variable to store the last time an image was acquired\n",
    "last_image_time = time.time()\n",
    "\n",
    "while True:\n",
    "    # Check if it's been at least 0.05 second since the last image acquisition\n",
    "    if time.time() - last_image_time < 0.05:\n",
    "        continue\n",
    "    vis.show()\n",
    "    path = vis.shortest_path\n",
    "    vis.update(path)\n",
    "\n",
    "    # Update the last image acquisition time\n",
    "    last_image_time = time.time()\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "del vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "828b87fe",
   "metadata": {},
   "source": [
    "### 3.1. Robot Detection and Localisation <a name=\"31-robot-detection-and-localisation\"></a>\n",
    "\n",
    "This is conducted throught the `find_robot()` function which operates solely on the defined vision class (as such this is the sole input). It accomplishes this through advanced image processing techniques, focusing on color filtering and contour detection, followed by a detailed analysis of the detected shapes.\n",
    "\n",
    "Firstly, through *colour filtering* the image is converted from its original BGR format to RGB. We then apply a colour filter to isolate the colour red, which is present on the thymio robot (as per our assumptions). This is achieved through defining a specific range in the RGB colour space that corresponds to the red colour, resulting in a binary mask where the red areas are highlighted. \n",
    "\n",
    "Then we undergo *contour detection* by using `cv2.findContours()` to detect the exact shape and structure of the labelling on the thymio. These contours, representing the boundaries of the red shapes are critical for the accurate determination of the robot's position. A specific range of contour areas are checked, to denoise the result and only get the two markers contours and positions. The contour area feature is also used to differentiate between the nose marker and the tail marker on the robot.\n",
    "\n",
    "This structure allows us to *calculate the centroids* by utilising the moments of the two contours. `cv2.moments()` allows for extraction of specific spatial moments required to compute the coordinates of the centroid which are calculated using the following formulae:\n",
    "\n",
    "$$C_x = \\frac{M_{10}}{M_{00}},\\; C_y = \\frac{M_{01}}{M_{00}}$$ \n",
    "\n",
    "The calculation of this centroid helps provide 5 critical pieces of information:\n",
    "1. **Central reference point**: Concise and accurate position of robot in environment, calculated from the average of the two markers position.\n",
    "2. **Orientation of robot**: The direction in which the robot is facing is inferred from the arctangent of the two centroids locations. \n",
    "3. **Scale Calibration**: By knowing the physical distance between the two marker centroids on the robot and comparing it with the pixel distance between their centroids in the image, you can establish a scale factor, providing the scale for all future calculations.\n",
    "4. **Movement Analysis**: The centroid can be tracked in sequential frames, thus, changes in the centroid's location over time provide information about the speed and direction of the robot's movement. \n",
    "5. **Path Planning Input**: The positional data derived from the centroids assist significantly in the path planning process, to be elaborated later in the report. \n",
    "\n",
    "Below is an image of the robot with its two markers and orientation detected:\n",
    "\n",
    "<img src=\"images/Robot.PNG\">\n",
    "\n",
    "The code in the cell below is the part of the vision code implementing the robot detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c612f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to find robot location and frame scale\n",
    "def find_robot(self):\n",
    "    # Convert the image from BGR to RGB (OpenCV loads images in BGR by default)\n",
    "    rgb_image = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Define the lower and upper bounds for the red color in RGB\n",
    "    lower_red = np.array([150, 20, 60])\n",
    "    upper_red = np.array([255, 100, 140])\n",
    "\n",
    "    # Create a binary mask using inRange function\n",
    "    red_mask = cv2.inRange(rgb_image, lower_red, upper_red)\n",
    "\n",
    "    # Apply the binary mask to the original image\n",
    "    result = cv2.bitwise_and(self.frame, self.frame, mask=red_mask)\n",
    "\n",
    "    # Convert the result to grayscale\n",
    "    gray_result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    gray_result = cv2.bilateralFilter(gray_result,5,15,15)\n",
    "    contours, _ = cv2.findContours(gray_result, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    min_area = 200\n",
    "    max_area = 10000\n",
    "    centroid = [0,0]\n",
    "\n",
    "    total = 0\n",
    "    nose_size = 0\n",
    "    # Draw the contours\n",
    "    for i, contour_i in enumerate(contours):\n",
    "        contour_area_i = cv2.contourArea(contour_i)\n",
    "        if min_area < contour_area_i < max_area:\n",
    "            total +=1\n",
    "            nose_size = max(nose_size, contour_area_i)\n",
    "\n",
    "    if total == 2:  #Here we have found the robot\n",
    "        for _, contour_i in enumerate(contours):\n",
    "            contour_area_i = cv2.contourArea(contour_i)\n",
    "\n",
    "            if min_area < contour_area_i < max_area:\n",
    "                # Selecting the nose side of the robot with i = 0\n",
    "                if contour_area_i == nose_size:\n",
    "                    i = 0\n",
    "                else:\n",
    "                    i = 1\n",
    "                color = (0, 0, 255)\n",
    "                cv2.drawContours(self.copy, [contour_i], 0, color, 2)\n",
    "                M = cv2.moments(contour_i)\n",
    "                centroid[i] = np.array([int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"])])\n",
    "        cv2.line(self.copy, centroid[0], centroid[1], (0,0,255),2)\n",
    "\n",
    "        # The real distance between the two markers on the robot is around 8.5 cm. so we set a scale\n",
    "        # to know how distant the objects are from the camera.\n",
    "        scale = np.linalg.norm(centroid[0] - centroid[1])/0.085\n",
    "\n",
    "        return True, Robot((centroid[0][0] + centroid[1][0])/2.0, (centroid[0][1] + centroid[1][1])/2.0,\n",
    "                    np.arctan2(centroid[0][1]-centroid[1][1],centroid[0][0]-centroid[1][0])), scale\n",
    "    else:\n",
    "        return False, Robot(0,0,0), 0 # Default value if robot not found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16acedac",
   "metadata": {},
   "source": [
    "### 3.2. Obstacle Detection <a name=\"32-obstacle-detection\"></a>\n",
    "This is achieved through the `find_graph()` function which again takes in the vision class as its sole input. Obstacle detection is achieved through Greyscale conversion, thresholding, contour approximation which blends together to contribute to the larger objective of creating a map for the robot to follow. \n",
    "\n",
    "The first step is *Greyscale Conversion* which significantly simplifies the process of identifying obstacles through `cv2.cvtColor()`. In greyscale, the intensity of each pixel represents different shades of grey, disregarding colour which is not needed to identify black (all obstacles are black as stated in assumptions). Once the image is rendered, *Thresholding* takes place which outputs an image where the black obstacles are distinctly visible against the background. \n",
    "\n",
    "In a similar process to above, the contours are detected but for obstacles, they are further simplifed in the process of *Contour Approximation* utilising `cv2.approxPolyDP()` to create the shape of the obstacles. This returns the object (obstacle) defined as a set of corners. This is done in order to balance between retaining the essential shape while reducing computational complexity. This serves to keep the algorithm efficient while maintaining accuracy. \n",
    "\n",
    "Finally, the boundaries of the actual obstacles and the robot have been defined. But in order to ensure that the path planning function is able to create a safe path through the obstacles, we conduct a *Vector Calculation* for Obstacles. \n",
    "This is done in `find_farthest_vector()` which takes in the set of corners for each of the obstacles, with the end goal being to create a safety net around obstacles to avoid collision. For each corner, this function calculates the sum of two vectors [previous_corner, corner] and [next_corner, coner] (in green and blue in the image below) and normalizes it. We then get the vector pointing outward relative to an obstacle for each corner of an obstacle (final vector in red in the image below).\n",
    "\n",
    "Then we can set the corners of each obstacles at a certain distance (determined using the scale factor obtained from the `find_robot()` function) along this vector from the obstacle. This helps create a buffer around the obstacles hence preventing the robot's path from getting too close to an obstacle. This also allows for the accomodation of any errors in measurement or movement.\n",
    "\n",
    "The image below illustrates the result of this part of the code.\n",
    "\n",
    "<img src=\"images/Obstacle_detection.PNG\" width=700 height=500>\n",
    "\n",
    "The code in the cell below is the part of the vision code implementing the obstacle detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the vector pointing outwards from a corner\n",
    "def find_vector_farthest(corner, corners):    \n",
    "    corners = corners.reshape(-1, 2)\n",
    "\n",
    "    # Find the index of the corner in the list of corners\n",
    "    corner_index = np.where((corners == corner).all(axis=1))[0][0]\n",
    "\n",
    "    # Get the total number of corners\n",
    "    num_corners = len(corners)\n",
    "\n",
    "    # Get the indices of the previous and next corners in a circular manner\n",
    "    prev_corner_index = (corner_index - 1) % num_corners\n",
    "    next_corner_index = (corner_index + 1) % num_corners\n",
    "\n",
    "    # Get the previous and next corners\n",
    "    prev_corner = corners[prev_corner_index]\n",
    "    current_corner = corners[corner_index]\n",
    "    next_corner = corners[next_corner_index]\n",
    "    \n",
    "    # Calculate vectors from the current corner to the previous and next corners\n",
    "    vector_prev = normal(np.float32(current_corner - prev_corner))\n",
    "    vector_next = normal(np.float32(current_corner - next_corner))\n",
    "    vector_farthest = normal(vector_prev + vector_next)\n",
    "\n",
    "    return vector_farthest\n",
    "\n",
    "# Function to return the normal of a vector\n",
    "def normal(v):\n",
    "    length = np.sqrt(v[0]**2+v[1]**2)\n",
    "    v[0] = v[0]/float(length)\n",
    "    v[1] = v[1]/float(length)\n",
    "    return v\n",
    "\n",
    "# Function to find the obstacles and make the visibility graph\n",
    "def find_graph(self):\n",
    "    #Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply some preprocessing\n",
    "    gray = cv2.bilateralFilter(gray,5,15,15)\n",
    "    # Apply a binary threshold to identify black pixels\n",
    "    _, binary = cv2.threshold(gray, 60, 255, cv2.THRESH_BINARY)\n",
    "    # Find contours in the binary image along with hierarchy\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Set contour area thresholds\n",
    "    min_area = 1000\n",
    "    max_area = 100000\n",
    "\n",
    "    # Approximation accuracy\n",
    "    epsilon = 0.025\n",
    "\n",
    "    # Store points and obstacles\n",
    "    points = []\n",
    "    obstacles = []\n",
    "    found_obstacles = False\n",
    "\n",
    "    for contour_i in contours:\n",
    "        contour_area_i = cv2.contourArea(contour_i)\n",
    "\n",
    "        if min_area < contour_area_i < max_area:\n",
    "            found_obstacles = True\n",
    "            # Draw the contours\n",
    "            color = (0, 255, 150)\n",
    "            cv2.drawContours(self.copy, [contour_i], 0, color, 2)\n",
    "\n",
    "            # Find the corners of the contour\n",
    "            corners = cv2.approxPolyDP(contour_i, epsilon * cv2.arcLength(contour_i, True), True)\n",
    "            obstacle_i = []\n",
    "\n",
    "            # Draw circles at each corner\n",
    "            for corner in corners:\n",
    "                # Find the vector pointing outward from the corner\n",
    "                vector_farthest = find_vector_farthest(corner[0], corners)\n",
    "                # Place a point at a distance of 13 cm from the corner using the \n",
    "                # SCALE we have found earlier from the robot\n",
    "                new_point = corner[0] + self.scale * 0.13 * vector_farthest\n",
    "                points.append(new_point)\n",
    "                obstacle_i.append((int(new_point[0]),int(new_point[1])))\n",
    "            if len(obstacle_i) >= 3:\n",
    "                obstacles.append(Polygon(obstacle_i))\n",
    "\n",
    "\n",
    "    # Build the visibility graph for the given points and obstacles\n",
    "    g = vg.VisGraph()\n",
    "\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5f3dd",
   "metadata": {},
   "source": [
    "### 3.3. Path Planning <a name=\"33-path-planning\"></a>\n",
    "The first step is to construct a visibility graph utilising `pyvisgraph` and general graph theory to visualise the corners of obstacles, robot's current position, and the goal. This is built in `find_graph` which goes through *Greyscale and Thresholding*, *Contour Detection and Polygon Approximation* and then *Vector Calculation for Safety Margins*. \n",
    "\n",
    "Once this processing is done, the space between obstacles is checked to ensure enough space for a thymio to travel through. For that, an algorithm checks if each polygon formed from the nodes around an obstacle intersect with other polygons. If two polygons are intersecting, they are combined into a bigger polygon, which the thymio will go around. This is done with the help of functions from the `shapely` library.\n",
    "\n",
    "Finally, the visibility graph is built with the help of the `pyvisgraph` library, by feeding it a list of non-intersecting polygons coordinates. This will determine what are all the neighbours of each node and store them in an object.\n",
    "\n",
    "The `find_shortest_path()` function in the Vision class plays a pivotal role in the robot's navigation system. Taking in the above map where there are the integrated outputs from the robot detection, obstacle detection, and goal identification processes to compute the optimal path for the robot to reach its goal using Djikstra's algorithm. \n",
    "\n",
    "Continuing on from the 3.2. `find_graph()` function, the next part implements the polygons intersection detection and visibility graph building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ...\n",
    "\n",
    "    # Build the visibility graph for the given points and obstacles\n",
    "        g = vg.VisGraph()\n",
    "\n",
    "    # Error catching because geometry of obstacles might be bad if a non-obstacle\n",
    "    # is detected\n",
    "    try:\n",
    "        if found_obstacles:\n",
    "            # Combining obstacles if they intersect\n",
    "            exclude_list = []\n",
    "            for i in range(len(obstacles)):\n",
    "                for j in range(i+1, len(obstacles)):\n",
    "                    if obstacles[i].intersects(obstacles[j]):\n",
    "                        exclude_list.append(i)\n",
    "                        obstacles[j] = unary_union([obstacles[i],obstacles[j]])\n",
    "\n",
    "            # Making a new list of non-intersecting polygons to use for graph making\n",
    "            new_obstacles = []\n",
    "            for i, obstacle in enumerate(obstacles):\n",
    "                if i not in exclude_list:\n",
    "                    coords = list(obstacle.exterior.coords)\n",
    "                    new_obstacles.append([vg.Point(coord[0], coord[1]) for coord in coords])\n",
    "\n",
    "            # Draw the points\n",
    "            for point in points:\n",
    "                cv2.circle(self.copy, point.astype(int), 5, (255, 0, 0), -1)\n",
    "            g.build(new_obstacles)\n",
    "    except:\n",
    "        return False, points, g\n",
    "    return found_obstacles, points, g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255c5bc",
   "metadata": {},
   "source": [
    "Below is a video illustrating the obstacle combination and path planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"videos/Obstacle_combination.mp4\" type=\"video/mp4\">\n",
    "  Error, can't play video\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c0ade5",
   "metadata": {},
   "source": [
    "### 3.4. Goal Identification <a name=\"34-goal-identification\"></a>\n",
    "The function `find_goal()` is utilised to identify the green marker which is placed at the end objective (as per assumptions). This function operates in the vision class and returns the centroid position of the goal. As part of the Vision class, this method ensures that the goal's location is consistently updated and integrated with the robot's navigation system.\n",
    "\n",
    "Similar to the Robot localisation, this function operates by first applying colour filtering to isolate green (as per assumptions about the goal colour). This mask is then applied to the image and contours are attached to accurately grasp the goal. Following the same process as the Robot Localisation above, centroids are calculated and the end goal of the robot is saved into the vision class, making it accessible for future calculations.\n",
    "\n",
    "Below is an image of the detected goal seen by the vision algorithm\n",
    "\n",
    "<img src=\"images/Goal.PNG\">\n",
    "\n",
    "And below is the code implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the goal location\n",
    "def find_goal(self):\n",
    "    # Convert the image from BGR to RGB\n",
    "    rgb_image = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Define the lower and upper bounds for the goal color in RGB (assuming green)\n",
    "    lower_green = np.array([75, 120, 105])\n",
    "    upper_green = np.array([95, 140, 125])\n",
    "\n",
    "    # Create a binary mask\n",
    "    green_mask = cv2.inRange(rgb_image, lower_green, upper_green)\n",
    "    # Apply some preprocessing\n",
    "    green_mask = cv2.bilateralFilter(green_mask,5,15,15)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(green_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 100:  # threshold for goal size\n",
    "            # Draw the contours\n",
    "            color = (0, 255, 0)\n",
    "            cv2.drawContours(self.copy, [contour], 0, color, 2)\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                return True, Goal(cx, cy)\n",
    "\n",
    "    return False, Goal(0, 0)  # Default value if goal not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fd688-367e-418d-989c-f398c229aad7",
   "metadata": {},
   "source": [
    "# 4. Filtering <a class=\"anchor\" id=\"Filtering\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c9780",
   "metadata": {},
   "source": [
    "The implementation of a Kalman filter is necessary to obtain smooth performance. It allows our robot to obtain a more precise estimate of its position. Furthermore, if at some point the vision loses the robot (or the robot is hidden), the position estimate will still get updated thanks to the motion model. However, our uncertainty on position will increase until a new camera measurement is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177ce02",
   "metadata": {},
   "source": [
    "### 4.1 The state space model <a name=\"41-the-state-space-model\"></a>\n",
    "The model is obtained through wheel odometry and the measurements through the output of the computer vision algorithm.\n",
    "A state space representation of our model can be given as follows:\n",
    "\\begin{equation}\n",
    "        x_t = \\begin{bmatrix}\n",
    "        1&0&0\\\\\n",
    "        0&1&0\\\\\n",
    "        0&0&1\\\\\n",
    "        \\end{bmatrix} x_{t-1} + \\begin{bmatrix}\n",
    "                                \\frac{R}{2}cos(x_{3,t-1})\\Delta t & \\frac{R}{2}cos(x_{3,t-1})\\Delta t\\\\\n",
    "                                \\frac{R}{2}sin(x_{3,t-1})\\Delta t & \\frac{R}{2}sin(x_{3,t-1})\\Delta t\\\\\n",
    "                                -\\frac{R}{d}\\Delta t & \\frac{R}{d}\\Delta t\\\\\n",
    "                                \\end{bmatrix} u\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "        y_t = \\begin{bmatrix}\n",
    "        1&0&0\\\\\n",
    "        0&1&0\\\\\n",
    "        0&0&1\\\\\n",
    "        \\end{bmatrix} x_t\n",
    "\\end{equation}\n",
    "\n",
    "with R the radius of the wheels, d the wheelbase of the thymio and:\n",
    "\n",
    "\\begin{equation}\n",
    "        x_t = \\begin{bmatrix}\n",
    "                x_{1,t}\\\\\n",
    "                x_{2,t}\\\\\n",
    "                x_{3,t}\\\\\n",
    "                \\end{bmatrix} = \\begin{bmatrix}\n",
    "                                x_t\\\\\n",
    "                                y_t\\\\\n",
    "                                \\theta_t\\\\\n",
    "                                \\end{bmatrix} \\text{and } u = \\begin{bmatrix}\n",
    "                                                                \\omega_r\\\\\n",
    "                                                                \\omega_l\\\\\n",
    "                                                                \\end{bmatrix}\n",
    "        \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b12d53",
   "metadata": {},
   "source": [
    "### 4.2 Filter choice and parameters <a name=\"42-filter-choice-and-parameters\"></a>\n",
    "We can see in the model above that our model has no non-linearities ($x_{3,t-1}$ is already known at time $t$ so $cos(x_{3,t-1})$ and $sin(x_{3,t-1})$ are scalars). We can therefore use a linear Kalman filter.\n",
    "\n",
    "We decided to use the KalmanFilter class from the filterpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee73ee",
   "metadata": {},
   "source": [
    "Find below snippets from the filtering.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f097a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import KalmanFilter\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146588e",
   "metadata": {},
   "source": [
    "We start by defining the filter's parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters\n",
    "dimension_x = 3 # State dimension\n",
    "dimension_z = 3 # Measurement dimension\n",
    "\n",
    "motor_scale = 43.52 # [Motor_space/(rad/s)] Scale of motor speeds in motor space to rad/s\n",
    "R = 0.021 # [m] The radius of the Thymio's wheels\n",
    "d = 0.095 # [m] The wheelbase of the Thymio\n",
    "dt = 0.137 # [s] Time delta between steps\n",
    "\n",
    "# Creating the filter\n",
    "f = KalmanFilter(dim_x=dimension_x, dim_z=dimension_z) # state and measurement variables are x, y and theta\n",
    "\n",
    "## Filter parameters\n",
    "# State transition matrix\n",
    "f.F = np.eye(3)        \n",
    "# Measurement function\n",
    "f.H = np.eye(3)\n",
    "# Initial covariance matrix\n",
    "f.P = np.eye(3) * 100\n",
    "# Measurement noise\n",
    "camera_variances = [2.13554018e-01, 2.93571267e-01, 6.02748876e-05]\n",
    "f.R = np.diag(camera_variances)\n",
    "# Process noise\n",
    "process_variances = [3.8751605996417765e-01, 3.8751605996417765e-01, 2.9656863710880975e-03]\n",
    "f.Q = np.diag(process_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1228fcd",
   "metadata": {},
   "source": [
    "The parameters above were defined as follows:\n",
    "- General parameters:\n",
    "    - R: Radius of the wheels, measured directly on the thymio.\n",
    "\n",
    "    - d: Wheelbase, measured directly on the thymio.\n",
    "\n",
    "    - dt: Average time between two calls, measured by running the code and keeping track of the time \n",
    "    between two calls of the filter.\n",
    "    \n",
    "    - Motor scale: Scale in $[\\text{motor space}\\cdot (\\frac{rad}{s})^{-1}]$ to convert motor velocities to $rad\\cdot s^{-1}$\n",
    "- Filter parameters:\n",
    "    - f.F: State transition matrix, identity matrix as seen in the model in 4.1.\n",
    "\n",
    "    - f.H: Measurement function, identity matrix as seen in the model in 4.1.\n",
    "\n",
    "    - f.P: Initial value of the position estimate covariance matrix. It is set to an arbitrarly large initial value so it converges once the first measurement is given.\n",
    "\n",
    "    - f.R: Covariance matrix of the measurement (output of vision algorithm). This was initially estimated by keeping the robot static and running the vision algorithm. We kept track of the returned position in a table and calculated the variance. This can be seen in `test_variance_camera.py`. The value obtained was later hand-tuned to obtain better results.\n",
    "    \n",
    "    - f.Q: Covariance matrix of the process (wheel odometry). This was initially estimated by having the robot advance with a constant speed command. We recorded the angular velocity of the wheels. We then translated each recorded angular velocity to a linear velocity in $x$, then multiplied by dt to get the change in $x$ over a constant dt. The variance of this change gives us the variance along $x$. We then assumed $\\text{Var}(x)=\\text{Var}(y)$. Finally, the variance along $\\theta$ was calculated similarly, by having the robot rotate at a constant rate. The obtained values were then hand-tuned to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f1a60",
   "metadata": {},
   "source": [
    "### 4.3 Running the filter <a name=\"43-running-the-filter\"></a>\n",
    "We can now present the `run_filter()` function. This function is called every time we want to run one iteration of the filter and obtain a position estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_filter(speed_right, speed_left, prev_angle, vis):\n",
    "    global f\n",
    "\n",
    "    camera_scale = vis.scale # [camera_coordinates/m]\n",
    "    # Converting the motors to rad/s\n",
    "    speed_right = speed_right / motor_scale\n",
    "    speed_left = speed_left / motor_scale\n",
    "    \n",
    "    # Defining control input and control transition matrix\n",
    "    u = np.array([[speed_right],\n",
    "                  [speed_left]])\n",
    "    B = np.array([[np.cos(prev_angle)*(dt/2), np.cos(prev_angle)*(dt/2)],\n",
    "                    [np.sin(prev_angle)*(dt/2), np.sin(prev_angle)*(dt/2)],\n",
    "                    [(-dt/d), (dt/d)]]) * R\n",
    "    \n",
    "    # Getting camera measurements and conveting to [m]\n",
    "    measurement = np.array([vis.robot.x/camera_scale, vis.robot.y/camera_scale, vis.robot.angle])\n",
    "    \n",
    "    # Predict step of kalman filter with control input\n",
    "    f.predict(u = u, B = B)\n",
    "    # Only update if we have new camera measurements\n",
    "    if vis.found_robot:\n",
    "        f.update(measurement)\n",
    "\n",
    "    # Defining the estimate in camera coordinates\n",
    "    estimate = np.array([f.x[0,0] * camera_scale, f.x[1,0] * camera_scale, f.x[2,0]])\n",
    "\n",
    "    # Plotting an ellipse to show evolution of uncertainty along x and y\n",
    "    cv2.ellipse(vis.copy, (int(estimate[0]), int(estimate[1])), (int(200 * f.P[0,0]), int(200 * f.P[1,1])), math.degrees(f.x[2,0]),0,360,(255,0,0),3)\n",
    "    # Plotting a line to show the angle estimate\n",
    "    cv2.line(vis.copy, (int(estimate[0]), int(estimate[1])), (int(estimate[0] + 100*np.cos(estimate[2])), int(estimate[1] + 100*np.sin(estimate[2]))), (255,0,0), 3)\n",
    "    \n",
    "    return estimate # Return the kalman filtered state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c75c3",
   "metadata": {},
   "source": [
    "The function performs calulcations in SI units so the first step is to convert `speed_right` and `speed_left` (the motor angular velocities) from motor space to SI using `motor_scale`. We can then initialize the control input `u` and control matrix `B` based on the motor angular velocities and the angle `prev_angle` obtained from the previous iteration of the filter. Afterwards we divide the $x$ and $y$ measurements by `camera_scale` to convert them from the camera's units to meters. The $\\theta$ measurement doesn't need to be scaled as the vision algorithm returns it in radians.\n",
    "\n",
    "We are now ready to run the predict step of the kalman filter using the `predict()` function from the `KalmanFilter` class. This function has optional arguments `u` and `B`, depending on whether the used model has a control input. Once the filter is done predicting, we use `vis.found_robot` to check if the vision algorithm has found the robot. If it has not, then we do not run the update step and will return the a priori position estimates, we also expect the covariance matrix `f.P` to grow. However, if the vision algorithm has seen the robot then we update the estimate with the measurements from the camera and return the a posteriori estimates.\n",
    "\n",
    "Before returning the position estimates we reconvert them to the camera coordinate space using `camera_scale` (once again, we do not need to convert the $\\theta$ estimate). The rest of the function plots a line representing the estimate of $\\theta$ and an ellipse representing the variance along $x$ and $y$. Note that the variances were scaled by a value of 200 to obtain a visible ellipse. The goal of this ellipse is simply to visualize the evolution in the variances as the filter runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed0db5",
   "metadata": {},
   "source": [
    "In the video below we hide the robot with a piece of paper. As expected, the filter's variance increases until a new measurement is acquired. We can see the filter is able to maintain a viable estimated position to keep the robot on its path. (reminder: the variance ellipse's size was scaled by 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"videos/filter.mp4\" type=\"video/mp4\">\n",
    "  Error, can't play video\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbebe13c-e5e8-407f-a02d-c6dc173859a3",
   "metadata": {},
   "source": [
    "# 5. General motion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76775246-9d5b-4d0e-bed2-20bef47f761d",
   "metadata": {},
   "source": [
    "When the program has been able to calculate the path the robot must take, it has a list of coordinates in memory, which are the positions the robot must pass through to reach the goal.\n",
    "\n",
    "Here we describe how the robot follows the trajectory from one point to another, without kidnapping and without unexpected obstacles in its path.\n",
    "\n",
    "Each time the robot passes through the loop, after the vision has been updated, the Kalman filter is used to update the robot's position estimate.\n",
    "\n",
    "We then check whether the robot's estimated position is approximately equal to its current target (the current node in the list containing the planned path), using the `close_coordinates(x, y, w, z)` function. This function compares two positions and returns True if they are approximately equal. If this is the case, the robot changes its objective, which becomes the next node in the list of nodes to visit.\n",
    "\n",
    "Next, the program checks whether the robot's estimated angle is approximately equal to the angle between its current position and its target (computed using the function `compute_movement(current_pos, obj)` ). If the difference is too great, the robot rotates until it is aligned. It then runs in a straight line.\n",
    "The code below is the implementation of what is described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea890a2-c744-4851-b6ed-c94fff0d17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while state == 1: # Drive towards goal state\n",
    "        while time.time()-last_image_time < dt: # Aquire a new image every dt seconds\n",
    "            continue\n",
    "        vis.show()\n",
    "        vis.update(path)\n",
    "        \n",
    "        await client.sleep(0.01)\n",
    "        await node.wait_for_variables()\n",
    "        \n",
    "        # Rest of state 1 #\n",
    "        \n",
    "        # Setting up and running the Kalman Filter\n",
    "        speed_left = node.v.motor.left.speed\n",
    "        speed_right = node.v.motor.right.speed\n",
    "        rob_pos = filtering.run_filter(speed_right, speed_left, rob_pos[2], vis)\n",
    "        \n",
    "        # Checking if we arrived at the next node\n",
    "        arrived_node = motion_functions.close_coordinates(rob_pos[0], rob_pos[1], path[target_node][0], path[target_node][1])\n",
    "        if (arrived_node):\n",
    "            if (target_node) < len(path)-1:\n",
    "                target_node += 1\n",
    "            else:\n",
    "                await motion_functions.stop(node)\n",
    "                goal_reached = True\n",
    "                break \n",
    "        \n",
    "        # Rotating and driving towards the next node\n",
    "        if rotation_done:\n",
    "            angle = motion_functions.compute_movement([rob_pos[0], rob_pos[1]], [path[target_node][0], path[target_node][1]])\n",
    "            if abs(angle - rob_pos[2]) > angle_tol: # If the difference in angle is smaller than angle_tol we stop turning\n",
    "                rotation_done = 0\n",
    "                await motion_functions.rotate(angle-rob_pos[2], node)\n",
    "            \n",
    "        if  angle + 0.2 > rob_pos[2] and angle - 0.2 < rob_pos[2]:\n",
    "            rotation_done = 1\n",
    "            await motion_functions.drive(node)\n",
    "        \n",
    "            \n",
    "        last_image_time = time.time()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0383bf-ebc6-4f6b-8b27-6589b2696083",
   "metadata": {},
   "source": [
    "# 6. Local avoidance <a class=\"anchor\" id=\"Local_avoidance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99aca8a-0b54-42f7-9281-adba969a574d",
   "metadata": {},
   "source": [
    "### 6.1 Requirements on the obstacles for local avoidance <a name=\"61-requirements-on-the-obstacles-for-local-avoidance\"></a>\n",
    "For local avoidance to work, obstacles must have the following characteristics: \n",
    "- The obstacles must be black:\n",
    "  - because path planning can only be carried out with black obstacles. If the obstacle placed is not black, the robot will detect it and move backwards, but when it recalculates its path, it will not take the new obstacle into account.\n",
    "- Measure more than 4 cm in height:\n",
    "   - otherwise the obstacle cannot be detected by the proximity sensors.\n",
    "\n",
    "### 6.2 Local avoidance implementation <a name=\"62-local-avoidance-implementation\"></a>\n",
    "Local avoidance is implemented as follows: \n",
    "If while the robot is moving towards its goal (in state 1), the value read by one of the horizontal proximity sensors is greater than a defined constant (THRESHOLD_OBSTACLE), a boolean variable obstacle_detected is set to True. The constant threshold obstacle has been set to 100 empirically. The function that updates this boolean is `get_sensors(node)`. When an obstacle is detected, the robot changes state to state 2.  \n",
    "The following cell contains the function get_sensors(node). This function is the same function that is used to identify whether or not the robot has been kidnapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c592219-cbfa-44b4-8822-79f8242e9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensors(node):\n",
    "    prox_values = node[\"prox.horizontal\"][:5]\n",
    "\n",
    "    obstacle_detected = any(value > THRESHOLD_OBSTACLE for value in prox_values)\n",
    "    \n",
    "    ground_values = node[\"prox.ground.reflected\"]\n",
    "    \n",
    "    if ground_values[0] < THRESHOLD_KIDNAPPED or ground_values[1] < THRESHOLD_KIDNAPPED:\n",
    "        kidnapped = True \n",
    "    else:\n",
    "        kidnapped = False\n",
    "    return kidnapped, obstacle_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e196b-eb03-449f-bbd0-40a33de34f18",
   "metadata": {},
   "source": [
    "The following cell contains the portion of the code that changes the state to state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf1f8f-77f8-4519-bb1d-f22363e174dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while state == 1: # Drive towards goal state\n",
    "    while time.time()-last_image_time < dt: # Aquire a new image every dt seconds\n",
    "        continue\n",
    "    vis.show()\n",
    "    vis.update(path)\n",
    "    \n",
    "    await client.sleep(0.01)\n",
    "    await node.wait_for_variables()\n",
    "    \n",
    "    # Get proximity sensors' values\n",
    "    kidnapped, obstacle_detected = motion_functions.get_sensors(node)\n",
    "    \n",
    "    # Checking for unforseen obstacle\n",
    "    if obstacle_detected:\n",
    "        await motion_functions.stop(node)\n",
    "        state = 2\n",
    "        break\n",
    "    \n",
    "    # Rest of state 1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec03b5d-74cb-4d68-8b46-9701dc36e358",
   "metadata": {},
   "source": [
    "As long as the robot is in state 2, the robot moves backwards in a straight line, until the proximity sensors no longer detect an obstacle.\n",
    "When the robot has moved back far enough, to the point where it no longer detects an obstacle, it changes state and returns to state 0. In this way, it recalculates its new path, taking into account the newly added obstacle. The program then returns to its general behavior.\n",
    "The following cell contains the state 2 of the robot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120b997-0fca-47dd-8892-c17979ff1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "while state == 2: # Local avoidance state\n",
    "        vis.shortest_path = [] # Deleting the previous path to make sure a new one is computed\n",
    "        \n",
    "        while obstacle_detected:\n",
    "            while time.time()-last_image_time < dt: # Aquire a new image every dt seconds\n",
    "                continue\n",
    "            vis.show()\n",
    "            vis.update(path)\n",
    "            \n",
    "            # Checking if we still detect obstacles\n",
    "            await client.sleep(0.01)\n",
    "            await node.wait_for_variables()\n",
    "            kidnapped, obstacle_detected = motion_functions.get_sensors(node)\n",
    "            # Drive backwards as long as an obstacle is detected\n",
    "            await motion_functions.drive_back(node)\n",
    "            \n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Stop and return to state 0 to compute a new path\n",
    "        await motion_functions.stop(node)\n",
    "        await client.sleep(0.5)  # Wait a bit to make sure our arm is not in the camera shot anymore as it creates noise.\n",
    "        state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359c8ce-5118-45db-b2af-e0681641fdb4",
   "metadata": {},
   "source": [
    "The cell below contains a video that illustrates local avoidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52446062-ac2f-4e9d-9716-47cefa358123",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"\"\"<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"videos/local_avoidance.mp4\" type=\"video/mp4\">\n",
    "  Error, can't play video\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e08c1-f790-4366-a765-0f3f02e71d9c",
   "metadata": {},
   "source": [
    "# 7. Kidnapping <a class=\"anchor\" id=\"Kidnapping\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad44ad7-09e9-484c-b1a0-f9ee15c755c8",
   "metadata": {},
   "source": [
    "Kidnapping is detected in a similar way to local avoidance. When the program is in state 1 (i.e. the robot is heading towards its goal), if the robot is abducted, the two sensors facing the ground determine whether the robot has been kidnapped. The function that handles this is `get_sensors(node)`. The sensors will return a small value, close to zero. If the value returned by the sensors is smaller than a defined threshold (SEUIL_KIDNAPPED), then the kidnapped Boolean variable is set to TRUE. In the state 1, if kidnapped is True, the motors stop turning, and the state is changed to state 3. The cell below shows the portion of code in state 1 changing the state to state 3 if the robot has been kidnapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644c310-16b3-4e8c-88dc-e6a6f2f061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while state == 1: # Drive towards goal state\n",
    "    while time.time()-last_image_time < dt: # Aquire a new image every dt seconds\n",
    "        continue\n",
    "    vis.show()\n",
    "    vis.update(path)\n",
    "    \n",
    "    await client.sleep(0.01)\n",
    "    await node.wait_for_variables()\n",
    "    \n",
    "    # Get proximity sensors' values\n",
    "    kidnapped, obstacle_detected = motion_functions.get_sensors(node)\n",
    "\n",
    "    # SOME CODE #\n",
    "\n",
    "    # Checking for kidnapping\n",
    "    if kidnapped:\n",
    "        state = 3 # Kidnapped state\n",
    "        vis.shortest_path = [] # Deleting the previous path to make sure a new one is computed\n",
    "        await motion_functions.stop(node)\n",
    "        break\n",
    "            \n",
    "    # Rest of state 1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f7bc1-8c18-4cbf-973f-682ada8cd082",
   "metadata": {},
   "source": [
    "While the robot is in state 3, the motors do not move and the values of its ground proximity sensors are read periodically using the function `get_sensors(node)`, so that if the sensors detect the ground again, the robot is returned to state 0. It will then recalculate its path and the program will resume normal operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b187e-7981-4f03-80dc-6f9c08cb3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "while state == 3: # State in case of kidnapping\n",
    "        while time.time()-last_image_time < dt: # Aquire a new image every dt seconds\n",
    "            continue\n",
    "        vis.show()\n",
    "        vis.update(path)\n",
    "        \n",
    "        # Checking if the robot is still kidnapped\n",
    "        await client.sleep(0.01)\n",
    "        await node.wait_for_variables()\n",
    "        kidnapped, obstacle_detected = motion_functions.get_sensors(node)\n",
    "        \n",
    "        if not kidnapped:\n",
    "            await client.sleep(0.75) # Wait a bit to make sure our arm is not in the camera shot anymore as it creates noise.\n",
    "            state = 0\n",
    "        \n",
    "        last_image_time = time.time()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633c34e-3c1e-4f7c-bb93-78489cf83f1c",
   "metadata": {},
   "source": [
    "The following cell contains a video that illustrates kidnapping behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc1c47-a31d-4479-93bb-2d6478f28cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"videos/kidnapping.mp4\" type=\"video/mp4\">\n",
    "  Error, can't play video\n",
    "</video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a5190-421d-40aa-98b9-3d1580c792ea",
   "metadata": {},
   "source": [
    "# 8. Conclusion <a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b9718-6715-4366-b6fc-813781588c2e",
   "metadata": {},
   "source": [
    "In summary, our project is centred on the development of an autonomous navigation system for a Thymio robot, melding computer vision with mobile robotics. The system navigates through a controlled environment, adeptly managing dynamic obstacle avoidance and recalibrating its path as required.\n",
    "\n",
    "We employed OpenCV and other libraries for efficient environment mapping and robot localisation, ensuring reliable navigation by precisely identifying obstacles and targets. The use of Pyvisgraph and Shapely enabled us to compute optimal paths and implement local avoidance algorithms, allowing the robot to effectively navigate around new obstacles and maintain its intended course.\n",
    "\n",
    "The system's resilience in handling unexpected 'kidnapping' scenarios, where the robot is displaced from its path, demonstrates its robustness. This was further emphasised by its rapid recalibration of routes post-disruption, showcasing its adaptability.\n",
    "\n",
    "The integration of a Kalman filter facilitates precise motion control, contributing to reliable navigation. Additionally, the asynchronous programming approach we adopted efficiently manages motor controls and sensor readings, ensuring optimal system responsiveness.\n",
    "\n",
    "This project furthermore taught us to work in a team where different each individual works on a different module. Using git and planning meetings was crucial to the success of our project. We learned how to effectively communicate and adapt our work to allow the different systems to work hand in hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
